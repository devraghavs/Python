{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a Pythonista\n"
     ]
    }
   ],
   "source": [
    "print(\"I'm a Pythonista\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-e960e1d4f1d5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-e960e1d4f1d5>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    if h <40:\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def computepay(h,r):\n",
    "if h <40:\n",
    "salary=h*r\n",
    "return salary\n",
    "else:\n",
    "salary=(h*r)+(h-40)*0.5*r\n",
    "return salary\n",
    "hrs = input(“Enter Hours:”)\n",
    "hrate= input(“enter the housrly rate”)\n",
    "h=float(hrs)\n",
    "r=float(hrate)\n",
    "p = computepay(h,r)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l=-1\n",
    "s=-1\n",
    "l=[]\n",
    "\n",
    "while True:\n",
    "    var=input(\"enter the number:\")\n",
    "    if var==\"done\" : \n",
    "        break\n",
    "    try:\n",
    "        ivar=int(var)\n",
    "        \n",
    "    except:\n",
    "        print(\"Invaild Input\")\n",
    "        continue\n",
    "    \n",
    "    l.append(var)\n",
    "print(l)\n",
    "mini=min(l)\n",
    "maxi=max(l)\n",
    "    \n",
    "print(\"Max:\",maxi,\"Min:\",mini)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest=None\n",
    "smallest=None\n",
    "\n",
    "while True:\n",
    "    num=input(\"enter the number:\")\n",
    "    if num==\"done\" : \n",
    "        break\n",
    "    try:\n",
    "        num = int(num)       \n",
    "    except:\n",
    "        print(\"Invaild input\")\n",
    "        continue\n",
    "    if largest is None or largest < num:\n",
    "            largest = num\n",
    "    elif smallest is None or smallest > num:\n",
    "            smallest = num\n",
    "    \n",
    "   \n",
    "    \n",
    "print (\"Maximum\", largest)\n",
    "print (\"Minimum\", smallest)        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest = None\n",
    "smallest = None\n",
    "while True:\n",
    "    try:\n",
    "        num = input(\"Enter a number: \")\n",
    "        if num == \"done\":\n",
    "            break\n",
    "        num = int(num)\n",
    "        if largest is None or largest < num:\n",
    "            largest = num\n",
    "        elif smallest is None or smallest > num:\n",
    "             smallest = num\n",
    "    except ValueError:\n",
    "        print(\"Invalid input\")\n",
    " \n",
    "print (\"Maximum\", largest)\n",
    "print (\"Minimum\", smallest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = input(\"Enter file name: \")\n",
    "lst = list()\n",
    "\n",
    "for line in fh:\n",
    "    line=line.rstrip()\n",
    "    #print(line)\n",
    "    #word=line.split()\n",
    "   # print(line)\n",
    "    word=line.split()\n",
    "    #lst.append(word)\n",
    "        #print(lst)\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[1,2,3,6,0]\n",
    "l.sort()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = input(\"Enter file name: \")\n",
    "if len(fname) < 1 : fname = \"mbox-short.txt\"\n",
    "\n",
    "fh = open(fname)\n",
    "count = 0\n",
    "for line in fh:\n",
    "    line=line.rstrip()\n",
    "    if not line.startswith('From'):\n",
    "        continue\n",
    "    if line.startswith('From:'):\n",
    "        continue\n",
    "    else:\n",
    "        words=line.split()\n",
    "        print(words[1])\n",
    "    count+=1\n",
    "    \n",
    "\n",
    "print(\"There were\", count, \"lines in the file with From as the first word\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = input(\"Enter file name: \")\n",
    "if len(fname) < 1 : fname = \"mbox-short.txt\"\n",
    "\n",
    "fh = open(fname)\n",
    "l=[]\n",
    "d={}\n",
    "for line in fh:\n",
    "    line=line.rstrip()\n",
    "    if not line.startswith('From'):\n",
    "        continue\n",
    "    if line.startswith('From:'):\n",
    "        continue\n",
    "    else:\n",
    "        words=line.split()\n",
    "        #print(words[5])\n",
    "        #l.append(words[5])\n",
    "        x=words[5].split(':')\n",
    "        print(x[0])\n",
    "        i=x[0]\n",
    "        #for i in x[0:2]:\n",
    "        d[i]=d.get(i,0)+1\n",
    "    \n",
    "        #l.append(x)\n",
    "       \t\n",
    "    \n",
    "#print(l)\n",
    "#print(d)\n",
    "c=(sorted([(k,v) for k,v in d.items()]))\n",
    "for k,v in sorted(d.items()):\n",
    "    print(k,v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cousre Python Web Access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=['arr']\n",
    "if re.search('arr',s):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "line=['Fromthe crowd is back in covid']\n",
    "for line in line:\n",
    "    line=line.rstrip()\n",
    "    if re.search('From',line):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find only numbers from the string:\n",
    "x='I have four no. 1,2,3,4'\n",
    "y=re.findall('[0-9]+',x)\n",
    "z=re.findall('[aeiou]+',x)\n",
    "print(\"the no. are\",y)\n",
    "print(\"the letter\",z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ letter.+...........it is a greedyapporach\n",
    "# ^ letter.+? ..........it is not a greedy approach\n",
    "x='From: the box we From aboutwthe ball:'\n",
    "y=re.findall('^F.+?:',x)\n",
    "z=re.findall('\\S+w\\S+',x)   #At least non-white space character\n",
    "a=re.findall('From (\\S+w\\S+)',x) # Starts from From and having the extraction (^ for staring one)\n",
    "print(a)\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='raghav@gamil.com rgre geg'\n",
    "y=re.findall('@([^ ]*)',x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "fh=open(\"F:\\\\Coursera\\\\Python\\\\sampleregex.txt\")\n",
    "f=fh.read()\n",
    "x=[]\n",
    "s=0\n",
    "for line in f:\n",
    "    line=line.rstrip()\n",
    "    y=re.findall('[0-9]+',line)\n",
    "    for i in y:\n",
    "        x.append(int(y))\n",
    "print(sum(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "line=['Fromthe 12,3432435 634654654 654654 654 6546456 54']\n",
    "x=[]\n",
    "s=0\n",
    "for l in line:\n",
    "    #line=line.rstrip()\n",
    "    y=re.findall('[0-9]+',l)\n",
    "    x=x+y\n",
    "    #print(x)\n",
    "for i in x:\n",
    "    s=s+int(i)\n",
    "print(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0 \n",
    "\n",
    "for line in file:\n",
    "    line = line.rstrip()\n",
    "    line = re.findall('([0-9]+)', line)\n",
    "    for i in line:\n",
    "        i = int(i)\n",
    "        sum += i    \n",
    "\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "fh=open(\"F:\\\\Coursera\\\\Python\\\\regex.txt\")\n",
    "ls=[]\n",
    "\n",
    "for x in fh: #create a list in the list\n",
    "   x=x.rstrip()\n",
    "   #print(x)\n",
    "   t= re.findall('[0-9]+',x) #all numbers\n",
    "   print(t)\n",
    "   for d in t: #for loop as there a empthy values in the list a\n",
    "        ls.append(int(d))\n",
    "print (sum(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "fh=open(\"F:\\\\Coursera\\\\Python\\\\regex.txt\")\n",
    "#=fh.read()\n",
    "x=[]\n",
    "#s=0\n",
    "for line in fh:\n",
    "    line=line.rstrip()\n",
    "    y=re.findall('[0-9]+',line)\n",
    "    for i in y:\n",
    "        x.append(int(i))\n",
    "print(sum(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "mysock=socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org',80))\n",
    "cmd='GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "while True:\n",
    "    data=mysock.recv(512)\n",
    "    if(len(data)<1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "mysock.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('e') #To find ASCII value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DYR-Don't Repeat Yourself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand= urllib.request.urlopen('http://data.pr4e.org/romeo.txt')#same as open a file\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Arise', 1), ('But', 1), ('It', 1), ('Juliet', 1), ('Who', 1), ('already', 1), ('and', 3), ('breaks', 1), ('east', 1), ('envious', 1), ('fair', 1), ('grief', 1), ('is', 3), ('kill', 1), ('light', 1), ('moon', 1), ('pale', 1), ('sick', 1), ('soft', 1), ('sun', 2), ('the', 3), ('through', 1), ('what', 1), ('window', 1), ('with', 1), ('yonder', 1)]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand= urllib.request.urlopen('http://data.pr4e.org/romeo.txt')#same as open a file\n",
    "d={}\n",
    "for line in fhand:\n",
    "    #print(line.decode().strip())\n",
    "    word=line.decode().split()\n",
    "    for w in word:\n",
    "        d[w]=d.get(w,0)+1\n",
    "print(sorted(d.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enterghjk\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown url type: 'ghjk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f6ee3712aeb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"enter\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mhtml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[1;31m# accept a URL or a Request object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullurl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullurl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfullurl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[0;32m    326\u001b[0m                  \u001b[0morigin_req_host\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munverifiable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                  method=None):\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munredirected_hdrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mfull_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfragment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeleter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown url type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplithost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unknown url type: 'ghjk'"
     ]
    }
   ],
   "source": [
    "#WEB crawlerrrrr..........\n",
    "\n",
    "import urllib.request, urllib.error,urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "url=input(\"enter\")\n",
    "html=urllib.request.urlopen(url).read()\n",
    "soup=BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "tags=soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href',None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEB crawlerrrrr..........\n",
    "\n",
    "import urllib.request, urllib.error,urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "ctx=ssl.create_default_context()\n",
    "ctx.check_hostname=False\n",
    "ctx.verify_mode=ssl.CERT_NONE\n",
    "url=input(\"enter\")\n",
    "html=urllib.request.urlopen(url,context=ctx).read()\n",
    "soup=BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "tags=soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href',None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('e')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "url = input('e')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "s=0\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "tags=soup(\"tr\")\n",
    "for tag in tags:\n",
    "    x=(tag.contents[0])\n",
    "    tags=soup(\"td\")\n",
    "    for x in tags:\n",
    "        #print(\"x\",x)\n",
    "        y=(x.contents[0])\n",
    "        print(y)\n",
    "    #s+=int(tag.contents[0])\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "count=int(input(\"Enter the count\"))\n",
    "position=int(input(\"Enter the position\"))\n",
    "\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "s=0\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for i in range(count):\n",
    "    l=tags[position].get('href',None)\n",
    "    #print(l)\n",
    "    print(tags[position].contents[0])\n",
    "    # Look at the parts of a tag\n",
    "    #print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    html = urlopen(l).read()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    href = soup('a')\n",
    "    #print('Contents:', tag.contents[0])\n",
    "    #print('Attrs:', tag.attrs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input('Enter URL:')\n",
    "count = int(input('Enter count:'))\n",
    "position = int(input('Enter position:'))-1\n",
    "html = urlopen(url).read()\n",
    "\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "tags = soup('a')\n",
    "\n",
    "for i in range(count):\n",
    "    l = tags[position].get('href', None)\n",
    "    print(l)\n",
    "    print(tags[position].contents[0])\n",
    "    html = urlopen(l).read()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    tags = soup('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "data='''\n",
    "<person>\n",
    "    <name>Raghav</name>\n",
    "    <phone type=\"intl\">+646499\n",
    "    </phone>\n",
    "</person>'''\n",
    "lst=[]\n",
    "tree=ET.fromstring(data)\n",
    "#print('Name',tree.find('name').text)\n",
    "#print('phone',tree.find('phone').text)\n",
    "lst=tree.findall('name')\n",
    "print(\"name\",len(lst))\n",
    "for item in lst:\n",
    "    print('name',item.find('name').text)\n",
    "    \n",
    " http://py4e-data.dr-chuck.net/comments_42.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import urllib.request as ur\n",
    "url = input('Enter - ')\n",
    "xml= ur.urlopen(url).read()\n",
    "tree=ET.fromstring(xml)\n",
    "#print('Name',tree.find('name').text)\n",
    "#print('phone',tree.find('phone').text)\n",
    "print(len(xml))\n",
    "lst=tree.findall('.//count')\n",
    "print(\"count\",len(lst))\n",
    "x=0\n",
    "for i in lst:\n",
    "    x+= int(i.text)\n",
    "print(\"Sum\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as ur\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "url = input('Enter ')\n",
    "t = 0\n",
    "sum = 0\n",
    "\n",
    "xml = ur.urlopen(url).read()\n",
    "print(len(xml))\n",
    "\n",
    "tree = et.fromstring(xml)\n",
    "counts = tree.findall('.//count')\n",
    "for count in counts:\n",
    "    sum+= int(count.text)\n",
    "    t+= 1\n",
    "\n",
    "print('Count:',t)\n",
    "print('Sum:', sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = '''\n",
    "[\n",
    "  { \"id\" : \"001\",\n",
    "    \"x\" : \"2\",\n",
    "    \"name\" : \"Chuck\"\n",
    "  } ,\n",
    "  { \"id\" : \"009\",\n",
    "    \"x\" : \"7\",\n",
    "    \"name\" : \"Brent\"\n",
    "  }\n",
    "]'''\n",
    "\n",
    "info = json.loads(data)\n",
    "print('User count:', len(info))\n",
    "\n",
    "for item in info:\n",
    "    print('Name', item['name'])\n",
    "    print('Id', item['id'])\n",
    "    print('Attribute', item['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "count = 0\n",
    "\n",
    "url =input(\"Enter the URl\")\n",
    "#print(\"retrieving URL. Stand by.\")\n",
    "uh = urlopen(url)\n",
    "data= uh.read()\n",
    "\n",
    "info = json.loads(data)\n",
    "for item in info[\"comments\"]:\n",
    "\n",
    "    number = int(item[\"count\"])\n",
    "    count = count + number\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEO LOCATION API // LOCATION ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "serviceurl = \"http://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "\n",
    "while True:\n",
    "\taddress = raw_input(\"Enter location:\")\n",
    "\tif len(address) <1: break\n",
    "\t\n",
    "\turl = serviceurl+ urllib.urlencode ({\"sensor\": \"false,\", \"address\":address})\n",
    "\tprint(\"retrieving\", url)\n",
    "\tuh = urllib.urlopen(url)\n",
    "\tdata = uh.read()\n",
    "\tprint(\"retrieved\", len(data), \"characters\"\n",
    "\ttry: js=json.loads(str(data))\n",
    "\texcept: js= None\n",
    "\tif \"status\" not in js or js['status'] != \"OK\":\n",
    "\t\tprint(\"=== failure to retrieve ===\")\n",
    "\t\tprint(data)\n",
    "\t\tcontinue\n",
    "\t\t\n",
    "\tprint(json.dumps(js, indent = 4)\n",
    "\t\n",
    "\tlat = js[\"results\"] [0] [\"geometry\"] [\"location\"][\"lat\"]\n",
    "\tlng = js[\"results\"] [0] [\"geometry\"] [\"location\"][\"lng\"]\n",
    "\tprint(\"lat\", lat, \"lng\", lng)\n",
    "\tlocation = js[\"results\"][0] [\"formatted_address\"]\n",
    "\tprint(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    #print(json.dumps(js, indent=4))\n",
    "\n",
    "    lat = js['results'][0]['geometry']['location']['lat']\n",
    "    lng = js['results'][0]['geometry']['location']['lng']\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    location = js['results'][0]['place_id']\n",
    "    print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('emaildb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('DROP TABLE IF EXISTS Counts')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE Counts (email TEXT, count INTEGER)''')\n",
    "\n",
    "url= \"http://www.pythonlearn.com/code/mbox.txt\"\n",
    "fh = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "\n",
    "for line in fh:\n",
    "    if not line.startswith('From: '): continue\n",
    "    pieces = line.split()\n",
    "    email = pieces[1]\n",
    "    cur.execute('SELECT count FROM Counts WHERE email = ? ', (email,))\n",
    "    row = cur.fetchone()\n",
    "    if row is None:\n",
    "        cur.execute('''INSERT INTO Counts (email, count)\n",
    "                VALUES (?, 1)''', (email,))\n",
    "    else:\n",
    "        cur.execute('UPDATE Counts SET count = count + 1 WHERE email = ?',\n",
    "                    (email,))\n",
    "    conn.commit()\n",
    "\n",
    "# https://www.sqlite.org/lang_select.html\n",
    "sqlstr = 'SELECT email, count FROM Counts ORDER BY count DESC LIMIT 10'\n",
    "\n",
    "for row in cur.execute(sqlstr):\n",
    "    print(str(row[0]), row[1])\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('emaildb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('DROP TABLE IF EXISTS Counts')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE Counts (org TEXT, count INTEGER)''')\n",
    "\n",
    "fname =\"F:\\Coursera\\Python\\mbox.txt\"\n",
    "if (len(fname) < 1): fname = 'mbox-short.txt'\n",
    "fh = open(fname)\n",
    "for line in fh:\n",
    "    if not line.startswith('From: '): continue\n",
    "    pieces = line.split()\n",
    "    email = pieces[1]\n",
    "    cur.execute('SELECT count FROM Counts WHERE org = ? ', (org, ))\n",
    "    row = cur.fetchone()\n",
    "    if row is None:\n",
    "        cur.execute('''INSERT INTO Counts (org, count)\n",
    "                VALUES (?, 1)''', (org,))\n",
    "    else:\n",
    "        cur.execute('UPDATE Counts SET count = count + 1 WHERE org = ?',\n",
    "                    (org,))\n",
    "    conn.commit()\n",
    "\n",
    "# https://www.sqlite.org/lang_select.html\n",
    "sqlstr = 'SELECT org, count FROM Counts ORDER BY count DESC LIMIT 10'\n",
    "\n",
    "for row in cur.execute(sqlstr):\n",
    "    print(str(row[0]), row[1])\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('emaildb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "#Deleting any possible table that may affect this assignment\n",
    "cur.execute('''\n",
    "DROP TABLE IF EXISTS Counts''')\n",
    "\n",
    "#Creating the table we're going to use\n",
    "cur.execute('''\n",
    "CREATE TABLE Counts (org TEXT, count INTEGER)''')\n",
    "\n",
    "#Indicating the file (URL in this case) from where we'll read the data\n",
    "fname =\"F:\\Coursera\\Python\\mbox.txt\" \n",
    "fh = open(fname)\n",
    "\n",
    "#Reading each line of the file\n",
    "for line in fh:\n",
    "\n",
    "    #Finding an email address and splitting it into name and organization\n",
    "    if not line.startswith('From:') : continue\n",
    "    pieces = line.split()\n",
    "    email = pieces[1]\n",
    "    (emailname, organization) = email.split(\"@\")\n",
    "    #print(email)\n",
    "\n",
    "    #Updating the table with the correspondent information\n",
    "    cur.execute('SELECT count FROM Counts WHERE org = ? ', (organization, ))\n",
    "    row = cur.fetchone()\n",
    "    if row is None:\n",
    "        cur.execute('''INSERT INTO Counts (org, count) \n",
    "                VALUES ( ?, 1 )''', ( organization, ) )\n",
    "    else : \n",
    "        cur.execute('UPDATE Counts SET count=count+1 WHERE org = ?', \n",
    "            (organization, ))\n",
    "\n",
    "# We commit the changes after they've finished because this speeds up the \n",
    "# execution and, since our operations are not critical, a loss wouldn't suppose\n",
    "# any problem\n",
    "conn.commit()\n",
    "\n",
    "# Getting the top 10 results and showing them\n",
    "sqlstr = 'SELECT org, count FROM Counts ORDER BY count DESC LIMIT 10'\n",
    "print (\"Counts:\")\n",
    "for row in cur.execute(sqlstr) :\n",
    "    print (str(row[0]), row[1])\n",
    "\n",
    "#Closing the DB\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "#Connecting to the file in which we want to store our db\n",
    "conn = sqlite3.connect('emaildb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "#Deleting any possible table that may affect this assignment\n",
    "cur.execute('''\n",
    "DROP TABLE IF EXISTS Counts''')\n",
    "\n",
    "#Creating the table we're going to use\n",
    "cur.execute('''\n",
    "CREATE TABLE Counts (org TEXT, count INTEGER)''')\n",
    "\n",
    "#Indicating the file (URL in this case) from where we'll read the data\n",
    "\n",
    "fhand= urllib.request.urlopen(\"http://www.pythonlearn.com/code/mbox.txt\")\n",
    "#Reading each line of the file\n",
    "for line in fhand:\n",
    "\n",
    "    #Finding an email address and splitting it into name and organization\n",
    "    if not line.startswith('From: ') : continue\n",
    "    pieces = line.split()\n",
    "    email = pieces[1]\n",
    "    (emailname, organization) = email.split(\"@\")\n",
    "\n",
    "\n",
    "    #Updating the table with the correspondent information\n",
    "    cur.execute('SELECT count FROM Counts WHERE org = ? ', (organization, ))\n",
    "    row = cur.fetchone()\n",
    "    if row is None:\n",
    "        cur.execute('''INSERT INTO Counts (org, count) \n",
    "                VALUES ( ?, 1 )''', ( organization, ) )\n",
    "    else : \n",
    "        cur.execute('UPDATE Counts SET count=count+1 WHERE org = ?', \n",
    "            (organization, ))\n",
    "\n",
    "# We commit the changes after they've finished because this speeds up the \n",
    "# execution and, since our operations are not critical, a loss wouldn't suppose\n",
    "# any problem\n",
    "conn.commit()\n",
    "\n",
    "# Getting the top 10 results and showing them\n",
    "sqlstr = 'SELECT org, count FROM Counts ORDER BY count DESC LIMIT 10'\n",
    "print\n",
    "print(\"Counts:\")\n",
    "for row in cur.execute(sqlstr) :\n",
    "    print (str(row[0]), row[1])\n",
    "\n",
    "#Closing the DB\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('trackdb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Make some fresh tables using executescript()\n",
    "cur.executescript('''\n",
    "DROP TABLE IF EXISTS Artist;\n",
    "DROP TABLE IF EXISTS Album;\n",
    "DROP TABLE IF EXISTS Track;\n",
    "DROP TABLE IF EXISTS Genre;\n",
    "\n",
    "\n",
    "CREATE TABLE Artist (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    name    TEXT UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE Genre (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    name    TEXT UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE Album (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    artist_id  INTEGER,\n",
    "    title   TEXT UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE Track (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY \n",
    "        AUTOINCREMENT UNIQUE,\n",
    "    title TEXT  UNIQUE,\n",
    "    album_id  INTEGER,\n",
    "    genre_id  INTEGER,\n",
    "    len INTEGER, rating INTEGER, count INTEGER\n",
    ");\n",
    "''')\n",
    "\n",
    "\n",
    "fname = input('Enter file name: ')\n",
    "if ( len(fname) < 1 ) : fname = 'Library.xml'\n",
    "\n",
    "# <key>Track ID</key><integer>369</integer>\n",
    "# <key>Name</key><string>Another One Bites The Dust</string>\n",
    "# <key>Artist</key><string>Queen</string>\n",
    "def lookup(d, key):\n",
    "    found = False\n",
    "    for child in d:\n",
    "        if found : return child.text\n",
    "        if child.tag == 'key' and child.text == key :\n",
    "            found = True\n",
    "    return None\n",
    "\n",
    "stuff = ET.parse(fname)\n",
    "all = stuff.findall('dict/dict/dict')\n",
    "print('Dict count:', len(all))\n",
    "for entry in all:\n",
    "    if ( lookup(entry, 'Track ID') is None ) : continue\n",
    "\n",
    "    name = lookup(entry, 'Name')\n",
    "    artist = lookup(entry, 'Artist')\n",
    "    album = lookup(entry, 'Album')\n",
    "    count = lookup(entry, 'Play Count')\n",
    "    rating = lookup(entry, 'Rating')\n",
    "    length = lookup(entry, 'Total Time')\n",
    "    genre= lookup(entry, 'Genre')\n",
    "\n",
    "    if name is None or artist is None or album is None : \n",
    "        continue\n",
    "\n",
    "    print(name, artist, album, count, rating, length)\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO Artist (name) \n",
    "        VALUES ( ? )''', ( artist, ) )\n",
    "    cur.execute('SELECT id FROM Artist WHERE name = ? ', (artist, ))\n",
    "    artist_id = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO Album (title, artist_id) \n",
    "        VALUES ( ?, ? )''', ( album, artist_id ) )\n",
    "    cur.execute('SELECT id FROM Album WHERE title = ? ', (album, ))\n",
    "    album_id = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute('''INSERT OR REPLACE INTO Track\n",
    "        (title, album_id, len, rating, count) \n",
    "        VALUES ( ?, ?, ?, ?, ? )''', \n",
    "        ( name, album_id, length, rating, count ) )\n",
    "\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('emaildb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('DROP TABLE IF EXISTS Counts')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE Counts (org TEXT, count INTEGER)''')\n",
    "\n",
    "fname = input('Enter file name: ')\n",
    "if (len(fname) < 1): fname = 'mbox-short.txt'\n",
    "fh = open(fname)\n",
    "for line in fh:\n",
    "    if not line.startswith('From: '): continue\n",
    "    pieces = line.split()[1]\n",
    "    org = pieces.split('@')[1]\n",
    "    cur.execute('SELECT count FROM Counts WHERE org = ? ', (org,))\n",
    "    row = cur.fetchone()\n",
    "    if row is None:\n",
    "        cur.execute('''INSERT INTO Counts (org, count)\n",
    "                VALUES (?, 1)''', (org,))\n",
    "    else:\n",
    "        cur.execute('UPDATE Counts SET count = count + 1 WHERE org = ?',\n",
    "                    (org,))\n",
    "conn.commit()\n",
    "\n",
    "# https://www.sqlite.org/lang_select.html\n",
    "sqlstr = 'SELECT org, count FROM Counts ORDER BY count DESC LIMIT 10'\n",
    "\n",
    "for row in cur.execute(sqlstr):\n",
    "    print(str(row[0]), row[1])\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('emaildb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''\n",
    "DROP TABLE IF EXISTS Counts''')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE Counts (org TEXT, count INTEGER)''')\n",
    "\n",
    "fname = raw_input('Enter file name: ')\n",
    "if ( len(fname) < 1 ) : fname = 'mbox.txt'\n",
    "fh = open(fname)\n",
    "for line in fh:\n",
    "\tif not line.startswith('From: ') : continue\n",
    "\tpieces = line.split()[1]\n",
    "\torg = pieces.split('@')[1]\n",
    "\t\n",
    "\t\n",
    "\tprint org\n",
    "\tcur.execute('SELECT count FROM Counts WHERE org = ? ', (org, ))\n",
    "\trow = cur.fetchone()\n",
    "\tif row is None:\n",
    "\t\tcur.execute('''INSERT INTO Counts (org, count) \n",
    "\t\t\t\tVALUES ( ?, 1 )''', ( org, ) )\n",
    "\telse : \n",
    "\t\tcur.execute('UPDATE Counts SET count=count+1 WHERE org = ?', \n",
    "\t\t\t(org, ))\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "sqlstr = 'SELECT org, count FROM Counts ORDER BY count DESC LIMIT 10'\n",
    "\n",
    "print\n",
    "print \"Counts:\"\n",
    "for row in cur.execute(sqlstr) :\n",
    "    print str(row[0]), row[1]\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('trackdb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Make some fresh tables using executescript()\n",
    "cur.executescript('''\n",
    "DROP TABLE IF EXISTS Artist;\n",
    "DROP TABLE IF EXISTS Genre;\n",
    "DROP TABLE IF EXISTS Album;\n",
    "DROP TABLE IF EXISTS Track;\n",
    "CREATE TABLE Artist (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    name    TEXT UNIQUE\n",
    ");\n",
    "CREATE TABLE Genre (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    name    TEXT UNIQUE\n",
    ");\n",
    "CREATE TABLE Album (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    artist_id  INTEGER,\n",
    "    title   TEXT UNIQUE\n",
    ");\n",
    "CREATE TABLE Track (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY \n",
    "        AUTOINCREMENT UNIQUE,\n",
    "    title TEXT  UNIQUE,\n",
    "    album_id  INTEGER,\n",
    "    genre_id INTEGER,\n",
    "    len INTEGER, rating INTEGER, count INTEGER\n",
    ");\n",
    "''')\n",
    "\n",
    "\n",
    "fname = input('Enter file name: ')\n",
    "if ( len(fname) < 1 ) : fname = 'Library.xml'\n",
    "\n",
    "# <key>Track ID</key><integer>369</integer>\n",
    "# <key>Name</key><string>Another One Bites The Dust</string>\n",
    "# <key>Artist</key><string>Queen</string>\n",
    "def lookup(d, key):\n",
    "    found = False\n",
    "    for child in d:\n",
    "        if found : return child.text\n",
    "        if child.tag == 'key' and child.text == key :\n",
    "            found = True\n",
    "    return None\n",
    "\n",
    "stuff = ET.parse(fname)\n",
    "all = stuff.findall('dict/dict/dict')\n",
    "for entry in all:\n",
    "    if ( lookup(entry, 'Track ID') is None ) : continue\n",
    "\n",
    "    name = lookup(entry, 'Name')\n",
    "    artist = lookup(entry, 'Artist')\n",
    "    album = lookup(entry, 'Album')\n",
    "    genre = lookup(entry, 'Genre')\n",
    "    count = lookup(entry, 'Play Count')\n",
    "    rating = lookup(entry, 'Rating')\n",
    "    length = lookup(entry, 'Total Time')\n",
    "\n",
    "    if name is None or artist is None or genre is None or album is None : \n",
    "        continue\n",
    "\n",
    "    print (name, artist, album, genre, count, rating, length)\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO Artist (name) \n",
    "        VALUES ( ? )''', ( artist, ) )\n",
    "    cur.execute('SELECT id FROM Artist WHERE name = ? ', (artist, ))\n",
    "    artist_id = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO Genre (name) \n",
    "    VALUES ( ? )''', ( genre, ) )\n",
    "    cur.execute('SELECT id FROM Genre WHERE name = ? ', (genre, ))\n",
    "    genre_id = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO Album (title, artist_id) \n",
    "        VALUES ( ?, ? )''', ( album, artist_id ) )\n",
    "    cur.execute('SELECT id FROM Album WHERE title = ? ', (album, ))\n",
    "    album_id = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute('''INSERT OR REPLACE INTO Track\n",
    "        (title, album_id, genre_id, len, rating, count) \n",
    "        VALUES ( ?, ?, ?, ?, ?, ? )''', \n",
    "        ( name, album_id, genre_id, length, rating, count ) )\n",
    "\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('rosterdb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Do some setup\n",
    "cur.executescript('''\n",
    "DROP TABLE IF EXISTS User;\n",
    "DROP TABLE IF EXISTS Member;\n",
    "DROP TABLE IF EXISTS Course;\n",
    "\n",
    "CREATE TABLE User (\n",
    "    id     INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    name   TEXT UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE Course (\n",
    "    id     INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    title  TEXT UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE Member (\n",
    "    user_id     INTEGER,\n",
    "    course_id   INTEGER,\n",
    "    role        INTEGER,\n",
    "    PRIMARY KEY (user_id, course_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "fname = input('Enter file name: ')\n",
    "if len(fname) < 1:\n",
    "    fname = 'roster_data_sample.json'\n",
    "\n",
    "# [\n",
    "#   [ \"Charley\", \"si110\", 1 ],\n",
    "#   [ \"Mea\", \"si110\", 0 ],\n",
    "\n",
    "str_data = open(fname).read()\n",
    "json_data = json.loads(str_data)\n",
    "\n",
    "for entry in json_data:\n",
    "\n",
    "    name = entry[0];\n",
    "    title = entry[1];\n",
    "    role= entry[2];\n",
    "\n",
    "    print((name, title,role))\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO User (name)\n",
    "        VALUES ( ? )''', ( name, ) )\n",
    "    cur.execute('SELECT id FROM User WHERE name = ? ', (name, ))\n",
    "    user_id = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO Course (title)\n",
    "        VALUES ( ? )''', ( title, ) )\n",
    "    cur.execute('SELECT id FROM Course WHERE title = ? ', (title, ))\n",
    "    course_id = cur.fetchone()[0]\n",
    "    \n",
    "\n",
    "    cur.execute('''INSERT OR REPLACE INTO Member\n",
    "        (user_id, course_id,role) VALUES ( ?, ? ,?)''',\n",
    "        ( user_id, course_id ,role) )\n",
    "    \n",
    "\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import http\n",
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "import ssl\n",
    "import sys\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = \"http://py4e-data.dr-chuck.net/json?\"\n",
    "else :\n",
    "    serviceurl = \"https://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "\n",
    "# Additional detail for urllib\n",
    "# http.client.HTTPConnection.debuglevel = 1\n",
    "\n",
    "conn = sqlite3.connect('geodata.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS Locations (address TEXT, geodata TEXT)''')\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "fh = open(\"where.data\")\n",
    "count = 0\n",
    "for line in fh:\n",
    "    if count > 200 :\n",
    "        print('Retrieved 200 locations, restart to retrieve more')\n",
    "        break\n",
    "\n",
    "    address = line.strip()\n",
    "    print('')\n",
    "    cur.execute(\"SELECT geodata FROM Locations WHERE address= ?\",\n",
    "        (memoryview(address.encode()), ))\n",
    "\n",
    "    try:\n",
    "        data = cur.fetchone()[0]\n",
    "        print(\"Found in database \",address)\n",
    "        continue\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    parms = dict()\n",
    "    parms[\"address\"] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters', data[:20].replace('\\n', ' '))\n",
    "    count = count + 1\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        print(data)  # We print in case unicode causes an error\n",
    "        continue\n",
    "\n",
    "    if 'status' not in js or (js['status'] != 'OK' and js['status'] != 'ZERO_RESULTS') :\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        break\n",
    "\n",
    "    cur.execute('''INSERT INTO Locations (address, geodata)\n",
    "            VALUES ( ?, ? )''', (memoryview(address.encode()), memoryview(data.encode()) ) )\n",
    "    conn.commit()\n",
    "    if count % 10 == 0 :\n",
    "        print('Pausing for a bit...')\n",
    "        time.sleep(5)\n",
    "\n",
    "print(\"Run geodump.py to read the data from the database so you can vizualize it on a map.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "conn = sqlite3.connect('geodata.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT * FROM Locations')\n",
    "fhand = codecs.open('where.js', 'w', \"utf-8\")\n",
    "fhand.write(\"myData = [\\n\")\n",
    "count = 0\n",
    "for row in cur :\n",
    "    data = str(row[1].decode())\n",
    "    try: js = json.loads(str(data))\n",
    "    except: continue\n",
    "\n",
    "    if not('status' in js and js['status'] == 'OK') : continue\n",
    "\n",
    "    lat = js[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
    "    lng = js[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n",
    "    if lat == 0 or lng == 0 : continue\n",
    "    where = js['results'][0]['formatted_address']\n",
    "    where = where.replace(\"'\", \"\")\n",
    "    try :\n",
    "        print(where, lat, lng)\n",
    "\n",
    "        count = count + 1\n",
    "        if count > 1 : fhand.write(\",\\n\")\n",
    "        output = \"[\"+str(lat)+\",\"+str(lng)+\", '\"+where+\"']\"\n",
    "        fhand.write(output)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "fhand.write(\"\\n];\\n\")\n",
    "cur.close()\n",
    "fhand.close()\n",
    "print(count, \"records written to where.js\")\n",
    "print(\"Open where.html to view the data in a browser\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zap = \"hello there bob\"\n",
    "print(zap[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=10\n",
    "if x < 2 :\n",
    "    print(\"Below 2\")\n",
    "elif x < 20 :\n",
    "    print(\"Below 20\")\n",
    "elif x < 10 :\n",
    "    print(\"Below 10\")\n",
    "else :\n",
    "    print(\"Something else\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "if x < 2 :\n",
    "    print(\"Below 2\")\n",
    "elif x < 0 :\n",
    "    print(\"Negative\")\n",
    "else :\n",
    "    print(\"Something else\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello():\n",
    "    print(\"Hello\")\n",
    "    print(\"There\")\n",
    "\n",
    "x = 10\n",
    "x = x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -1\n",
    "for value in [3, 41, 12, 9, 74, 15] :\n",
    "    if value > x :\n",
    "        x = value\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for abc in range(5):\n",
    "    total = total + abc\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"123\"\n",
    "b = 456\n",
    "c = a + b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"With three words\"\n",
    "stuff = abc.split()\n",
    "print(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()#spider.py\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "        conn.commit()\n",
    "\n",
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        # print href\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "\t\t# Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')  #Sprank\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Find the ids that send out page rank - we only are interested\n",
    "# in pages in the SCC that have in and out links\n",
    "cur.execute('''SELECT DISTINCT from_id FROM Links''')\n",
    "from_ids = list()\n",
    "for row in cur: \n",
    "    from_ids.append(row[0])\n",
    "\n",
    "# Find the ids that receive page rank \n",
    "to_ids = list()\n",
    "links = list()\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "for row in cur:\n",
    "    from_id = row[0]\n",
    "    to_id = row[1]\n",
    "    if from_id == to_id : continue\n",
    "    if from_id not in from_ids : continue\n",
    "    if to_id not in from_ids : continue\n",
    "    links.append(row)\n",
    "    if to_id not in to_ids : to_ids.append(to_id)\n",
    "\n",
    "# Get latest page ranks for strongly connected component\n",
    "prev_ranks = dict()\n",
    "for node in from_ids:\n",
    "    cur.execute('''SELECT new_rank FROM Pages WHERE id = ?''', (node, ))\n",
    "    row = cur.fetchone()\n",
    "    prev_ranks[node] = row[0]\n",
    "\n",
    "sval = input('How many iterations:')\n",
    "many = 1\n",
    "if ( len(sval) > 0 ) : many = int(sval)\n",
    "\n",
    "# Sanity check\n",
    "if len(prev_ranks) < 1 : \n",
    "    print(\"Nothing to page rank.  Check data.\")\n",
    "    quit()\n",
    "\n",
    "# Lets do Page Rank in memory so it is really fast\n",
    "for i in range(many):\n",
    "    # print prev_ranks.items()[:5]\n",
    "    next_ranks = dict();\n",
    "    total = 0.0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        total = total + old_rank\n",
    "        next_ranks[node] = 0.0\n",
    "    # print total\n",
    "\n",
    "    # Find the number of outbound links and sent the page rank down each\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        # print node, old_rank\n",
    "        give_ids = list()\n",
    "        for (from_id, to_id) in links:\n",
    "            if from_id != node : continue\n",
    "           #  print '   ',from_id,to_id\n",
    "\n",
    "            if to_id not in to_ids: continue\n",
    "            give_ids.append(to_id)\n",
    "        if ( len(give_ids) < 1 ) : continue\n",
    "        amount = old_rank / len(give_ids)\n",
    "        # print node, old_rank,amount, give_ids\n",
    "    \n",
    "        for id in give_ids:\n",
    "            next_ranks[id] = next_ranks[id] + amount\n",
    "    \n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "    evap = (total - newtot) / len(next_ranks)\n",
    "\n",
    "    # print newtot, evap\n",
    "    for node in next_ranks:\n",
    "        next_ranks[node] = next_ranks[node] + evap\n",
    "\n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "\n",
    "    # Compute the per-page average change from old rank to new rank\n",
    "    # As indication of convergence of the algorithm\n",
    "    totdiff = 0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        new_rank = next_ranks[node]\n",
    "        diff = abs(old_rank-new_rank)\n",
    "        totdiff = totdiff + diff\n",
    "\n",
    "    avediff = totdiff / len(prev_ranks)\n",
    "    print(i+1, avediff)\n",
    "\n",
    "    # rotate\n",
    "    prev_ranks = next_ranks\n",
    "\n",
    "# Put the final ranks back into the database\n",
    "print(list(next_ranks.items())[:5])\n",
    "cur.execute('''UPDATE Pages SET old_rank=new_rank''')\n",
    "for (id, new_rank) in list(next_ranks.items()) :\n",
    "    cur.execute('''UPDATE Pages SET new_rank=? WHERE id=?''', (new_rank, id))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')# spdump\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "     FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "     WHERE html IS NOT NULL\n",
    "     GROUP BY id ORDER BY inbound DESC''')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    if count < 25 : print(row)\n",
    "    count = count + 1\n",
    "print(count, 'rows.')\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import ssl\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Not all systems have this so conditionally define parser\n",
    "try:\n",
    "    import dateutil.parser as parser\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def parsemaildate(md) :\n",
    "    # See if we have dateutil\n",
    "    try:\n",
    "        pdate = parser.parse(tdate)\n",
    "        test_at = pdate.isoformat()\n",
    "        return test_at\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Non-dateutil version - we try our best\n",
    "\n",
    "    pieces = md.split()\n",
    "    notz = \" \".join(pieces[:4]).strip()\n",
    "\n",
    "    # Try a bunch of format variations - strptime() is *lame*\n",
    "    dnotz = None\n",
    "    for form in [ '%d %b %Y %H:%M:%S', '%d %b %Y %H:%M:%S',\n",
    "        '%d %b %Y %H:%M', '%d %b %Y %H:%M', '%d %b %y %H:%M:%S',\n",
    "        '%d %b %y %H:%M:%S', '%d %b %y %H:%M', '%d %b %y %H:%M' ] :\n",
    "        try:\n",
    "            dnotz = datetime.strptime(notz, form)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if dnotz is None :\n",
    "        # print 'Bad Date:',md\n",
    "        return None\n",
    "\n",
    "    iso = dnotz.isoformat()\n",
    "\n",
    "    tz = \"+0000\"\n",
    "    try:\n",
    "        tz = pieces[4]\n",
    "        ival = int(tz) # Only want numeric timezone values\n",
    "        if tz == '-0000' : tz = '+0000'\n",
    "        tzh = tz[:3]\n",
    "        tzm = tz[3:]\n",
    "        tz = tzh+\":\"+tzm\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return iso+tz\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('content.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "baseurl = \"http://mbox.dr-chuck.net/sakai.devel/\"\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Messages\n",
    "    (id INTEGER UNIQUE, email TEXT, sent_at TEXT,\n",
    "     subject TEXT, headers TEXT, body TEXT)''')\n",
    "\n",
    "# Pick up where we left off\n",
    "start = None\n",
    "cur.execute('SELECT max(id) FROM Messages' )\n",
    "try:\n",
    "    row = cur.fetchone()\n",
    "    if row is None :\n",
    "        start = 0\n",
    "    else:\n",
    "        start = row[0]\n",
    "except:\n",
    "    start = 0\n",
    "\n",
    "if start is None : start = 0\n",
    "\n",
    "many = 0\n",
    "count = 0\n",
    "fail = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        conn.commit()\n",
    "        sval = input('How many messages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "\n",
    "    start = start + 1\n",
    "    cur.execute('SELECT id FROM Messages WHERE id=?', (start,) )\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        if row is not None : continue\n",
    "    except:\n",
    "        row = None\n",
    "\n",
    "    many = many - 1\n",
    "    url = baseurl + str(start) + '/' + str(start + 1)\n",
    "\n",
    "    text = \"None\"\n",
    "    try:\n",
    "        # Open with a timeout of 30 seconds\n",
    "        document = urllib.request.urlopen(url, None, 30, context=ctx)\n",
    "        text = document.read().decode()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error code=\",document.getcode(), url)\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Unable to retrieve or parse page\",url)\n",
    "        print(\"Error\",e)\n",
    "        fail = fail + 1\n",
    "        if fail > 5 : break\n",
    "        continue\n",
    "\n",
    "    print(url,len(text))\n",
    "    count = count + 1\n",
    "\n",
    "    if not text.startswith(\"From \"):\n",
    "        print(text)\n",
    "        print(\"Did not find From \")\n",
    "        fail = fail + 1\n",
    "        if fail > 5 : break\n",
    "        continue\n",
    "\n",
    "    pos = text.find(\"\\n\\n\")\n",
    "    if pos > 0 :\n",
    "        hdr = text[:pos]\n",
    "        body = text[pos+2:]\n",
    "    else:\n",
    "        print(text)\n",
    "        print(\"Could not find break between headers and body\")\n",
    "        fail = fail + 1\n",
    "        if fail > 5 : break\n",
    "        continue\n",
    "\n",
    "    email = None\n",
    "    x = re.findall('\\nFrom: .* <(\\S+@\\S+)>\\n', hdr)\n",
    "    if len(x) == 1 :\n",
    "        email = x[0];\n",
    "        email = email.strip().lower()\n",
    "        email = email.replace(\"<\",\"\")\n",
    "    else:\n",
    "        x = re.findall('\\nFrom: (\\S+@\\S+)\\n', hdr)\n",
    "        if len(x) == 1 :\n",
    "            email = x[0];\n",
    "            email = email.strip().lower()\n",
    "            email = email.replace(\"<\",\"\")\n",
    "\n",
    "    date = None\n",
    "    y = re.findall('\\Date: .*, (.*)\\n', hdr)\n",
    "    if len(y) == 1 :\n",
    "        tdate = y[0]\n",
    "        tdate = tdate[:26]\n",
    "        try:\n",
    "            sent_at = parsemaildate(tdate)\n",
    "        except:\n",
    "            print(text)\n",
    "            print(\"Parse fail\",tdate)\n",
    "            fail = fail + 1\n",
    "            if fail > 5 : break\n",
    "            continue\n",
    "\n",
    "    subject = None\n",
    "    z = re.findall('\\Subject: (.*)\\n', hdr)\n",
    "    if len(z) == 1 : subject = z[0].strip().lower();\n",
    "\n",
    "    # Reset the fail counter\n",
    "    fail = 0\n",
    "    print(\"   \",email,sent_at,subject)\n",
    "    cur.execute('''INSERT OR IGNORE INTO Messages (id, email, sent_at, subject, headers, body)\n",
    "        VALUES ( ?, ?, ?, ?, ?, ? )''', ( start, email, sent_at, subject, hdr, body))\n",
    "    if count % 50 == 0 : conn.commit()\n",
    "    if count % 100 == 0 : time.sleep(1)\n",
    "\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import re\n",
    "import zlib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Not all systems have this\n",
    "try:\n",
    "    import dateutil.parser as parser\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dnsmapping = dict()\n",
    "mapping = dict()\n",
    "\n",
    "def fixsender(sender,allsenders=None) :\n",
    "    global dnsmapping\n",
    "    global mapping\n",
    "    if sender is None : return None\n",
    "    sender = sender.strip().lower()\n",
    "    sender = sender.replace('<','').replace('>','')\n",
    "\n",
    "    # Check if we have a hacked gmane.org from address\n",
    "    if allsenders is not None and sender.endswith('gmane.org') :\n",
    "        pieces = sender.split('-')\n",
    "        realsender = None\n",
    "        for s in allsenders:\n",
    "            if s.startswith(pieces[0]) :\n",
    "                realsender = sender\n",
    "                sender = s\n",
    "                # print(realsender, sender)\n",
    "                break\n",
    "        if realsender is None :\n",
    "            for s in mapping:\n",
    "                if s.startswith(pieces[0]) :\n",
    "                    realsender = sender\n",
    "                    sender = mapping[s]\n",
    "                    # print(realsender, sender)\n",
    "                    break\n",
    "        if realsender is None : sender = pieces[0]\n",
    "\n",
    "    mpieces = sender.split(\"@\")\n",
    "    if len(mpieces) != 2 : return sender\n",
    "    dns = mpieces[1]\n",
    "    x = dns\n",
    "    pieces = dns.split(\".\")\n",
    "    if dns.endswith(\".edu\") or dns.endswith(\".com\") or dns.endswith(\".org\") or dns.endswith(\".net\") :\n",
    "        dns = \".\".join(pieces[-2:])\n",
    "    else:\n",
    "        dns = \".\".join(pieces[-3:])\n",
    "    # if dns != x : print(x,dns)\n",
    "    # if dns != dnsmapping.get(dns,dns) : print(dns,dnsmapping.get(dns,dns))\n",
    "    dns = dnsmapping.get(dns,dns)\n",
    "    return mpieces[0] + '@' + dns\n",
    "\n",
    "def parsemaildate(md) :\n",
    "    # See if we have dateutil\n",
    "    try:\n",
    "        pdate = parser.parse(md)\n",
    "        test_at = pdate.isoformat()\n",
    "        return test_at\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Non-dateutil version - we try our best\n",
    "\n",
    "    pieces = md.split()\n",
    "    notz = \" \".join(pieces[:4]).strip()\n",
    "\n",
    "    # Try a bunch of format variations - strptime() is *lame*\n",
    "    dnotz = None\n",
    "    for form in [ '%d %b %Y %H:%M:%S', '%d %b %Y %H:%M:%S',\n",
    "        '%d %b %Y %H:%M', '%d %b %Y %H:%M', '%d %b %y %H:%M:%S',\n",
    "        '%d %b %y %H:%M:%S', '%d %b %y %H:%M', '%d %b %y %H:%M' ] :\n",
    "        try:\n",
    "            dnotz = datetime.strptime(notz, form)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if dnotz is None :\n",
    "        # print('Bad Date:',md)\n",
    "        return None\n",
    "\n",
    "    iso = dnotz.isoformat()\n",
    "\n",
    "    tz = \"+0000\"\n",
    "    try:\n",
    "        tz = pieces[4]\n",
    "        ival = int(tz) # Only want numeric timezone values\n",
    "        if tz == '-0000' : tz = '+0000'\n",
    "        tzh = tz[:3]\n",
    "        tzm = tz[3:]\n",
    "        tz = tzh+\":\"+tzm\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return iso+tz\n",
    "\n",
    "# Parse out the info...\n",
    "def parseheader(hdr, allsenders=None):\n",
    "    if hdr is None or len(hdr) < 1 : return None\n",
    "    sender = None\n",
    "    x = re.findall('\\nFrom: .* <(\\S+@\\S+)>\\n', hdr)\n",
    "    if len(x) >= 1 :\n",
    "        sender = x[0]\n",
    "    else:\n",
    "        x = re.findall('\\nFrom: (\\S+@\\S+)\\n', hdr)\n",
    "        if len(x) >= 1 :\n",
    "            sender = x[0]\n",
    "\n",
    "    # normalize the domain name of Email addresses\n",
    "    sender = fixsender(sender, allsenders)\n",
    "\n",
    "    date = None\n",
    "    y = re.findall('\\nDate: .*, (.*)\\n', hdr)\n",
    "    sent_at = None\n",
    "    if len(y) >= 1 :\n",
    "        tdate = y[0]\n",
    "        tdate = tdate[:26]\n",
    "        try:\n",
    "            sent_at = parsemaildate(tdate)\n",
    "        except Exception as e:\n",
    "            # print('Date ignored ',tdate, e)\n",
    "            return None\n",
    "\n",
    "    subject = None\n",
    "    z = re.findall('\\nSubject: (.*)\\n', hdr)\n",
    "    if len(z) >= 1 : subject = z[0].strip().lower()\n",
    "\n",
    "    guid = None\n",
    "    z = re.findall('\\nMessage-ID: (.*)\\n', hdr)\n",
    "    if len(z) >= 1 : guid = z[0].strip().lower()\n",
    "\n",
    "    if sender is None or sent_at is None or subject is None or guid is None :\n",
    "        return None\n",
    "    return (guid, sender, subject, sent_at)\n",
    "\n",
    "conn = sqlite3.connect('index.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS Messages ''')\n",
    "cur.execute('''DROP TABLE IF EXISTS Senders ''')\n",
    "cur.execute('''DROP TABLE IF EXISTS Subjects ''')\n",
    "cur.execute('''DROP TABLE IF EXISTS Replies ''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Messages\n",
    "    (id INTEGER PRIMARY KEY, guid TEXT UNIQUE, sent_at INTEGER,\n",
    "     sender_id INTEGER, subject_id INTEGER,\n",
    "     headers BLOB, body BLOB)''')\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Senders\n",
    "    (id INTEGER PRIMARY KEY, sender TEXT UNIQUE)''')\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Subjects\n",
    "    (id INTEGER PRIMARY KEY, subject TEXT UNIQUE)''')\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Replies\n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "conn_1 = sqlite3.connect('mapping.sqlite')\n",
    "cur_1 = conn_1.cursor()\n",
    "\n",
    "cur_1.execute('''SELECT old,new FROM DNSMapping''')\n",
    "for message_row in cur_1 :\n",
    "    dnsmapping[message_row[0].strip().lower()] = message_row[1].strip().lower()\n",
    "\n",
    "mapping = dict()\n",
    "cur_1.execute('''SELECT old,new FROM Mapping''')\n",
    "for message_row in cur_1 :\n",
    "    old = fixsender(message_row[0])\n",
    "    new = fixsender(message_row[1])\n",
    "    mapping[old] = fixsender(new)\n",
    "\n",
    "# Done with mapping.sqlite\n",
    "conn_1.close()\n",
    "\n",
    "# Open the main content (Read only)\n",
    "conn_1 = sqlite3.connect('file:content.sqlite?mode=ro', uri=True)\n",
    "cur_1 = conn_1.cursor()\n",
    "\n",
    "allsenders = list()\n",
    "cur_1.execute('''SELECT email FROM Messages''')\n",
    "for message_row in cur_1 :\n",
    "    sender = fixsender(message_row[0])\n",
    "    if sender is None : continue\n",
    "    if 'gmane.org' in sender : continue\n",
    "    if sender in allsenders: continue\n",
    "    allsenders.append(sender)\n",
    "\n",
    "print(\"Loaded allsenders\",len(allsenders),\"and mapping\",len(mapping),\"dns mapping\",len(dnsmapping))\n",
    "\n",
    "cur_1.execute('''SELECT headers, body, sent_at\n",
    "    FROM Messages ORDER BY sent_at''')\n",
    "\n",
    "senders = dict()\n",
    "subjects = dict()\n",
    "guids = dict()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for message_row in cur_1 :\n",
    "    hdr = message_row[0]\n",
    "    parsed = parseheader(hdr, allsenders)\n",
    "    if parsed is None: continue\n",
    "    (guid, sender, subject, sent_at) = parsed\n",
    "\n",
    "    # Apply the sender mapping\n",
    "    sender = mapping.get(sender,sender)\n",
    "\n",
    "    count = count + 1\n",
    "    if count % 250 == 1 : print(count,sent_at, sender)\n",
    "    # print(guid, sender, subject, sent_at)\n",
    "\n",
    "    if 'gmane.org' in sender:\n",
    "        print(\"Error in sender ===\", sender)\n",
    "\n",
    "    sender_id = senders.get(sender,None)\n",
    "    subject_id = subjects.get(subject,None)\n",
    "    guid_id = guids.get(guid,None)\n",
    "\n",
    "    if sender_id is None :\n",
    "        cur.execute('INSERT OR IGNORE INTO Senders (sender) VALUES ( ? )', ( sender, ) )\n",
    "        conn.commit()\n",
    "        cur.execute('SELECT id FROM Senders WHERE sender=? LIMIT 1', ( sender, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            sender_id = row[0]\n",
    "            senders[sender] = sender_id\n",
    "        except:\n",
    "            print('Could not retrieve sender id',sender)\n",
    "            break\n",
    "    if subject_id is None :\n",
    "        cur.execute('INSERT OR IGNORE INTO Subjects (subject) VALUES ( ? )', ( subject, ) )\n",
    "        conn.commit()\n",
    "        cur.execute('SELECT id FROM Subjects WHERE subject=? LIMIT 1', ( subject, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            subject_id = row[0]\n",
    "            subjects[subject] = subject_id\n",
    "        except:\n",
    "            print('Could not retrieve subject id',subject)\n",
    "            break\n",
    "    # print(sender_id, subject_id)\n",
    "    cur.execute('INSERT OR IGNORE INTO Messages (guid,sender_id,subject_id,sent_at,headers,body) VALUES ( ?,?,?,datetime(?),?,? )',\n",
    "            ( guid, sender_id, subject_id, sent_at,\n",
    "            zlib.compress(message_row[0].encode()), zlib.compress(message_row[1].encode())) )\n",
    "    conn.commit()\n",
    "    cur.execute('SELECT id FROM Messages WHERE guid=? LIMIT 1', ( guid, ))\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        message_id = row[0]\n",
    "        guids[guid] = message_id\n",
    "    except:\n",
    "        print('Could not retrieve guid id',guid)\n",
    "        break\n",
    "\n",
    "cur.close()\n",
    "cur_1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import zlib\n",
    "\n",
    "howmany = int(input(\"How many to dump? \"))\n",
    "\n",
    "conn = sqlite3.connect('index.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT id, sender FROM Senders')\n",
    "senders = dict()\n",
    "for message_row in cur :\n",
    "    senders[message_row[0]] = message_row[1]\n",
    "\n",
    "cur.execute('SELECT id, subject FROM Subjects')\n",
    "subjects = dict()\n",
    "for message_row in cur :\n",
    "    subjects[message_row[0]] = message_row[1]\n",
    "\n",
    "# cur.execute('SELECT id, guid,sender_id,subject_id,headers,body FROM Messages')\n",
    "cur.execute('SELECT id, guid,sender_id,subject_id,sent_at FROM Messages')\n",
    "messages = dict()\n",
    "for message_row in cur :\n",
    "    messages[message_row[0]] = (message_row[1],message_row[2],message_row[3],message_row[4])\n",
    "\n",
    "print(\"Loaded messages=\",len(messages),\"subjects=\",len(subjects),\"senders=\",len(senders))\n",
    "\n",
    "sendcounts = dict()\n",
    "sendorgs = dict()\n",
    "for (message_id, message) in list(messages.items()):\n",
    "    sender = message[1]\n",
    "    sendcounts[sender] = sendcounts.get(sender,0) + 1\n",
    "    pieces = senders[sender].split(\"@\")\n",
    "    if len(pieces) != 2 : continue\n",
    "    dns = pieces[1]\n",
    "    sendorgs[dns] = sendorgs.get(dns,0) + 1\n",
    "\n",
    "print('')\n",
    "print('Top',howmany,'Email list participants')\n",
    "\n",
    "x = sorted(sendcounts, key=sendcounts.get, reverse=True)\n",
    "for k in x[:howmany]:\n",
    "    print(senders[k], sendcounts[k])\n",
    "    if sendcounts[k] < 10 : break\n",
    "\n",
    "print('')\n",
    "print('Top',howmany,'Email list organizations')\n",
    "\n",
    "x = sorted(sendorgs, key=sendorgs.get, reverse=True)\n",
    "for k in x[:howmany]:\n",
    "    print(k, sendorgs[k])\n",
    "    if sendorgs[k] < 10 : break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import zlib\n",
    "import string\n",
    "\n",
    "conn = sqlite3.connect('index.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT id, subject FROM Subjects')\n",
    "subjects = dict()\n",
    "for message_row in cur :\n",
    "    subjects[message_row[0]] = message_row[1]\n",
    "\n",
    "# cur.execute('SELECT id, guid,sender_id,subject_id,headers,body FROM Messages')\n",
    "cur.execute('SELECT subject_id FROM Messages')\n",
    "counts = dict()\n",
    "for message_row in cur :\n",
    "    text = subjects[message_row[0]]\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    text = text.translate(str.maketrans('','','1234567890'))\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if len(word) < 4 : continue\n",
    "        counts[word] = counts.get(word,0) + 1\n",
    "\n",
    "x = sorted(counts, key=counts.get, reverse=True)\n",
    "highest = None\n",
    "lowest = None\n",
    "for k in x[:100]:\n",
    "    if highest is None or highest < counts[k] :\n",
    "        highest = counts[k]\n",
    "    if lowest is None or lowest > counts[k] :\n",
    "        lowest = counts[k]\n",
    "print('Range of counts:',highest,lowest)\n",
    "\n",
    "# Spread the font sizes across 20-100 based on the count\n",
    "bigsize = 80\n",
    "smallsize = 20\n",
    "\n",
    "fhand = open('gword.js','w')\n",
    "fhand.write(\"gword = [\")\n",
    "first = True\n",
    "for k in x[:100]:\n",
    "    if not first : fhand.write( \",\\n\")\n",
    "    first = False\n",
    "    size = counts[k]\n",
    "    size = (size - lowest) / float(highest - lowest)\n",
    "    size = int((size * bigsize) + smallsize)\n",
    "    fhand.write(\"{text: '\"+k+\"', size: \"+str(size)+\"}\")\n",
    "fhand.write( \"\\n];\\n\")\n",
    "fhand.close()\n",
    "\n",
    "print(\"Output written to gword.js\")\n",
    "print(\"Open gword.htm in a browser to see the vizualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import zlib\n",
    "\n",
    "conn = sqlite3.connect('index.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT id, sender FROM Senders')\n",
    "senders = dict()\n",
    "for message_row in cur :\n",
    "    senders[message_row[0]] = message_row[1]\n",
    "\n",
    "cur.execute('SELECT id, guid,sender_id,subject_id,sent_at FROM Messages')\n",
    "messages = dict()\n",
    "for message_row in cur :\n",
    "    messages[message_row[0]] = (message_row[1],message_row[2],message_row[3],message_row[4])\n",
    "\n",
    "print(\"Loaded messages=\",len(messages),\"senders=\",len(senders))\n",
    "\n",
    "sendorgs = dict()\n",
    "for (message_id, message) in list(messages.items()):\n",
    "    sender = message[1]\n",
    "    pieces = senders[sender].split(\"@\")\n",
    "    if len(pieces) != 2 : continue\n",
    "    dns = pieces[1]\n",
    "    sendorgs[dns] = sendorgs.get(dns,0) + 1\n",
    "\n",
    "# pick the top schools\n",
    "orgs = sorted(sendorgs, key=sendorgs.get, reverse=True)\n",
    "orgs = orgs[:10]\n",
    "print(\"Top 10 Organizations\")\n",
    "print(orgs)\n",
    "\n",
    "counts = dict()\n",
    "months = list()\n",
    "# cur.execute('SELECT id, guid,sender_id,subject_id,sent_at FROM Messages')\n",
    "for (message_id, message) in list(messages.items()):\n",
    "    sender = message[1]\n",
    "    pieces = senders[sender].split(\"@\")\n",
    "    if len(pieces) != 2 : continue\n",
    "    dns = pieces[1]\n",
    "    if dns not in orgs : continue\n",
    "    month = message[3][:7]\n",
    "    if month not in months : months.append(month)\n",
    "    key = (month, dns)\n",
    "    counts[key] = counts.get(key,0) + 1\n",
    "\n",
    "months.sort()\n",
    "# print counts\n",
    "# print months\n",
    "\n",
    "fhand = open('gline.js','w')\n",
    "fhand.write(\"gline = [ ['Month'\")\n",
    "for org in orgs:\n",
    "    fhand.write(\",'\"+org+\"'\")\n",
    "fhand.write(\"]\")\n",
    "\n",
    "for month in months:\n",
    "    fhand.write(\",\\n['\"+month+\"'\")\n",
    "    for org in orgs:\n",
    "        key = (month, org)\n",
    "        val = counts.get(key,0)\n",
    "        fhand.write(\",\"+str(val))\n",
    "    fhand.write(\"]\");\n",
    "\n",
    "fhand.write(\"\\n];\\n\")\n",
    "fhand.close()\n",
    "\n",
    "print(\"Output written to gline.js\")\n",
    "print(\"Open gline.htm to visualize the data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import zlib\n",
    "\n",
    "conn = sqlite3.connect('index.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT id, sender FROM Senders')\n",
    "senders = dict()\n",
    "for message_row in cur :\n",
    "    senders[message_row[0]] = message_row[1]\n",
    "\n",
    "cur.execute('SELECT id, guid,sender_id,subject_id,sent_at FROM Messages')\n",
    "messages = dict()\n",
    "for message_row in cur :\n",
    "    messages[message_row[0]] = (message_row[1],message_row[2],message_row[3],message_row[4])\n",
    "\n",
    "print(\"Loaded messages=\",len(messages),\"senders=\",len(senders))\n",
    "\n",
    "sendorgs = dict()\n",
    "for (message_id, message) in list(messages.items()):\n",
    "    sender = message[1]\n",
    "    pieces = senders[sender].split(\"@\")\n",
    "    if len(pieces) != 2 : continue\n",
    "    dns = pieces[1]\n",
    "    sendorgs[dns] = sendorgs.get(dns,0) + 1\n",
    "\n",
    "# pick the top schools\n",
    "orgs = sorted(sendorgs, key=sendorgs.get, reverse=True)\n",
    "orgs = orgs[:10]\n",
    "print(\"Top 10 Organizations\")\n",
    "print(orgs)\n",
    "\n",
    "counts = dict()\n",
    "years = list()\n",
    "# cur.execute('SELECT id, guid,sender_id,subject_id,sent_at FROM Messages')\n",
    "for (message_id, message) in list(messages.items()):\n",
    "    sender = message[1]\n",
    "    pieces = senders[sender].split(\"@\")\n",
    "    if len(pieces) != 2 : continue\n",
    "    dns = pieces[1]\n",
    "    if dns not in orgs : continue\n",
    "    year = message[3][:7]\n",
    "    if year not in years : years.append(year)\n",
    "    key = (year, dns)\n",
    "    counts[key] = counts.get(key,0) + 1\n",
    "\n",
    "years.sort()\n",
    "# print counts\n",
    "# print years\n",
    "\n",
    "fhand = open('gline.js','w')\n",
    "fhand.write(\"gline = [ ['year'\")\n",
    "for org in orgs:\n",
    "    fhand.write(\",'\"+org+\"'\")\n",
    "fhand.write(\"]\")\n",
    "\n",
    "for year in years:\n",
    "    fhand.write(\",\\n['\"+year+\"'\")\n",
    "    for org in orgs:\n",
    "        key = (year, org)\n",
    "        val = counts.get(key,0)\n",
    "        fhand.write(\",\"+str(val))\n",
    "    fhand.write(\"]\");\n",
    "\n",
    "fhand.write(\"\\n];\\n\")\n",
    "fhand.close()\n",
    "\n",
    "print(\"Output written to gline.js\")\n",
    "print(\"Open gline.htm to visualize the data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import zlib\n",
    "\n",
    "conn = sqlite3.connect('index.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT id, sender FROM Senders')\n",
    "senders = dict()\n",
    "for message_row in cur :\n",
    "    senders[message_row[0]] = message_row[1]\n",
    "\n",
    "cur.execute('SELECT id, guid,sender_id,subject_id,sent_at FROM Messages')\n",
    "messages = dict()\n",
    "for message_row in cur :\n",
    "    messages[message_row[0]] = (message_row[1],message_row[2],message_row[3],message_row[4])\n",
    "\n",
    "print(\"Loaded messages=\",len(messages),\"senders=\",len(senders))\n",
    "\n",
    "sendorgs = dict()\n",
    "for (message_id, message) in list(messages.items()):\n",
    "    sender = message[1]\n",
    "    pieces = senders[sender].split(\"@\")\n",
    "    if len(pieces) != 2 : continue\n",
    "    dns = pieces[1]\n",
    "    sendorgs[dns] = sendorgs.get(dns,0) + 1\n",
    "\n",
    "# pick the top schools\n",
    "orgs = sorted(sendorgs, key=sendorgs.get, reverse=True)\n",
    "orgs = orgs[:10]\n",
    "print(\"Top 10 Organizations\")\n",
    "print(orgs)\n",
    "# orgs = ['total'] + orgs\n",
    "\n",
    "counts = dict()\n",
    "months = list()\n",
    "# cur.execute('SELECT id, guid,sender_id,subject_id,sent_at FROM Messages')\n",
    "for (message_id, message) in list(messages.items()):\n",
    "    sender = message[1]\n",
    "    pieces = senders[sender].split(\"@\")\n",
    "    if len(pieces) != 2 : continue\n",
    "    dns = pieces[1]\n",
    "    if dns not in orgs : continue\n",
    "    month = message[3][:4]\n",
    "    if month not in months : months.append(month)\n",
    "    key = (month, dns)\n",
    "    counts[key] = counts.get(key,0) + 1\n",
    "    tkey = (month, 'total')\n",
    "    counts[tkey] = counts.get(tkey,0) + 1\n",
    "    \n",
    "months.sort()\n",
    "# print counts\n",
    "# print months\n",
    "\n",
    "fhand = open('gline.js','w')\n",
    "fhand.write(\"gline = [ ['Year'\")\n",
    "for org in orgs:\n",
    "    fhand.write(\",'\"+org+\"'\")\n",
    "fhand.write(\"]\")\n",
    "\n",
    "for month in months[1:-1]:\n",
    "    fhand.write(\",\\n['\"+month+\"'\")\n",
    "    for org in orgs:\n",
    "        key = (month, org)\n",
    "        val = counts.get(key,0)\n",
    "        fhand.write(\",\"+str(val))\n",
    "    fhand.write(\"]\");\n",
    "\n",
    "fhand.write(\"\\n];\\n\")\n",
    "fhand.close()\n",
    "\n",
    "print(\"Output written to gline.js\")\n",
    "print(\"Open gline.htm to visualize the data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "        conn.commit()\n",
    "\n",
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        # print href\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "\t\t# Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "cur.close()#spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Find the ids that send out page rank - we only are interested\n",
    "# in pages in the SCC that have in and out links\n",
    "cur.execute('''SELECT DISTINCT from_id FROM Links''')\n",
    "from_ids = list()\n",
    "for row in cur: \n",
    "    from_ids.append(row[0])\n",
    "\n",
    "# Find the ids that receive page rank \n",
    "to_ids = list()\n",
    "links = list()\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "for row in cur:\n",
    "    from_id = row[0]\n",
    "    to_id = row[1]\n",
    "    if from_id == to_id : continue\n",
    "    if from_id not in from_ids : continue\n",
    "    if to_id not in from_ids : continue\n",
    "    links.append(row)\n",
    "    if to_id not in to_ids : to_ids.append(to_id)\n",
    "\n",
    "# Get latest page ranks for strongly connected component\n",
    "prev_ranks = dict()\n",
    "for node in from_ids:\n",
    "    cur.execute('''SELECT new_rank FROM Pages WHERE id = ?''', (node, ))\n",
    "    row = cur.fetchone()\n",
    "    prev_ranks[node] = row[0]\n",
    "\n",
    "sval = input('How many iterations:')\n",
    "many = 1\n",
    "if ( len(sval) > 0 ) : many = int(sval)\n",
    "\n",
    "# Sanity check\n",
    "if len(prev_ranks) < 1 : \n",
    "    print(\"Nothing to page rank.  Check data.\")\n",
    "    quit()\n",
    "\n",
    "# Lets do Page Rank in memory so it is really fast\n",
    "for i in range(many):\n",
    "    # print prev_ranks.items()[:5]\n",
    "    next_ranks = dict();\n",
    "    total = 0.0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        total = total + old_rank\n",
    "        next_ranks[node] = 0.0\n",
    "    # print total\n",
    "\n",
    "    # Find the number of outbound links and sent the page rank down each\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        # print node, old_rank\n",
    "        give_ids = list()\n",
    "        for (from_id, to_id) in links:\n",
    "            if from_id != node : continue\n",
    "           #  print '   ',from_id,to_id\n",
    "\n",
    "            if to_id not in to_ids: continue\n",
    "            give_ids.append(to_id)\n",
    "        if ( len(give_ids) < 1 ) : continue\n",
    "        amount = old_rank / len(give_ids)\n",
    "        # print node, old_rank,amount, give_ids\n",
    "    \n",
    "        for id in give_ids:\n",
    "            next_ranks[id] = next_ranks[id] + amount\n",
    "    \n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "    evap = (total - newtot) / len(next_ranks)\n",
    "\n",
    "    # print newtot, evap\n",
    "    for node in next_ranks:\n",
    "        next_ranks[node] = next_ranks[node] + evap\n",
    "\n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "\n",
    "    # Compute the per-page average change from old rank to new rank\n",
    "    # As indication of convergence of the algorithm\n",
    "    totdiff = 0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        new_rank = next_ranks[node]\n",
    "        diff = abs(old_rank-new_rank)\n",
    "        totdiff = totdiff + diff\n",
    "\n",
    "    avediff = totdiff / len(prev_ranks)\n",
    "    print(i+1, avediff)\n",
    "\n",
    "    # rotate\n",
    "    prev_ranks = next_ranks\n",
    "\n",
    "# Put the final ranks back into the database\n",
    "print(list(next_ranks.items())[:5])\n",
    "cur.execute('''UPDATE Pages SET old_rank=new_rank''')\n",
    "for (id, new_rank) in list(next_ranks.items()) :\n",
    "    cur.execute('''UPDATE Pages SET new_rank=? WHERE id=?''', (new_rank, id))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "     FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "     WHERE html IS NOT NULL\n",
    "     GROUP BY id ORDER BY inbound DESC''')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    if count < 50 : print(row)\n",
    "    count = count + 1\n",
    "print(count, 'rows.')\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Creating JSON output on spider.js...\")\n",
    "howmany = int(input(\"How many nodes? \"))\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "    FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "    WHERE html IS NOT NULL AND ERROR IS NULL\n",
    "    GROUP BY id ORDER BY id,inbound''')\n",
    "\n",
    "fhand = open('spider.js','w')\n",
    "nodes = list()\n",
    "maxrank = None\n",
    "minrank = None\n",
    "for row in cur :\n",
    "    nodes.append(row)\n",
    "    rank = row[2]\n",
    "    if maxrank is None or maxrank < rank: maxrank = rank\n",
    "    if minrank is None or minrank > rank : minrank = rank\n",
    "    if len(nodes) > howmany : break\n",
    "\n",
    "if maxrank == minrank or maxrank is None or minrank is None:\n",
    "    print(\"Error - please run sprank.py to compute page rank\")\n",
    "    quit()\n",
    "\n",
    "fhand.write('spiderJson = {\"nodes\":[\\n')\n",
    "count = 0\n",
    "map = dict()\n",
    "ranks = dict()\n",
    "for row in nodes :\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    # print row\n",
    "    rank = row[2]\n",
    "    rank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{'+'\"weight\":'+str(row[0])+',\"rank\":'+str(rank)+',')\n",
    "    fhand.write(' \"id\":'+str(row[3])+', \"url\":\"'+row[4]+'\"}')\n",
    "    map[row[3]] = count\n",
    "    ranks[row[3]] = rank\n",
    "    count = count + 1\n",
    "fhand.write('],\\n')\n",
    "\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "fhand.write('\"links\":[\\n')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    # print row\n",
    "    if row[0] not in map or row[1] not in map : continue\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    rank = ranks[row[0]]\n",
    "    srank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{\"source\":'+str(map[row[0]])+',\"target\":'+str(map[row[1]])+',\"value\":3}')\n",
    "    count = count + 1\n",
    "fhand.write(']};')\n",
    "fhand.close()\n",
    "cur.close()\n",
    "\n",
    "print(\"Open force.html in a browser to view the visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider2.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "        conn.commit()\n",
    "\n",
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        # print href\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "\t\t# Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "cur.close()#spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider2.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Find the ids that send out page rank - we only are interested\n",
    "# in pages in the SCC that have in and out links\n",
    "cur.execute('''SELECT DISTINCT from_id FROM Links''')\n",
    "from_ids = list()\n",
    "for row in cur: \n",
    "    from_ids.append(row[0])\n",
    "\n",
    "# Find the ids that receive page rank \n",
    "to_ids = list()\n",
    "links = list()\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "for row in cur:\n",
    "    from_id = row[0]\n",
    "    to_id = row[1]\n",
    "    if from_id == to_id : continue\n",
    "    if from_id not in from_ids : continue\n",
    "    if to_id not in from_ids : continue\n",
    "    links.append(row)\n",
    "    if to_id not in to_ids : to_ids.append(to_id)\n",
    "\n",
    "# Get latest page ranks for strongly connected component\n",
    "prev_ranks = dict()\n",
    "for node in from_ids:\n",
    "    cur.execute('''SELECT new_rank FROM Pages WHERE id = ?''', (node, ))\n",
    "    row = cur.fetchone()\n",
    "    prev_ranks[node] = row[0]\n",
    "\n",
    "sval = input('How many iterations:')\n",
    "many = 1\n",
    "if ( len(sval) > 0 ) : many = int(sval)\n",
    "\n",
    "# Sanity check\n",
    "if len(prev_ranks) < 1 : \n",
    "    print(\"Nothing to page rank.  Check data.\")\n",
    "    quit()\n",
    "\n",
    "# Lets do Page Rank in memory so it is really fast\n",
    "for i in range(many):\n",
    "    # print prev_ranks.items()[:5]\n",
    "    next_ranks = dict();\n",
    "    total = 0.0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        total = total + old_rank\n",
    "        next_ranks[node] = 0.0\n",
    "    # print total\n",
    "\n",
    "    # Find the number of outbound links and sent the page rank down each\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        # print node, old_rank\n",
    "        give_ids = list()\n",
    "        for (from_id, to_id) in links:\n",
    "            if from_id != node : continue\n",
    "           #  print '   ',from_id,to_id\n",
    "\n",
    "            if to_id not in to_ids: continue\n",
    "            give_ids.append(to_id)\n",
    "        if ( len(give_ids) < 1 ) : continue\n",
    "        amount = old_rank / len(give_ids)\n",
    "        # print node, old_rank,amount, give_ids\n",
    "    \n",
    "        for id in give_ids:\n",
    "            next_ranks[id] = next_ranks[id] + amount\n",
    "    \n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "    evap = (total - newtot) / len(next_ranks)\n",
    "\n",
    "    # print newtot, evap\n",
    "    for node in next_ranks:\n",
    "        next_ranks[node] = next_ranks[node] + evap\n",
    "\n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "\n",
    "    # Compute the per-page average change from old rank to new rank\n",
    "    # As indication of convergence of the algorithm\n",
    "    totdiff = 0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        new_rank = next_ranks[node]\n",
    "        diff = abs(old_rank-new_rank)\n",
    "        totdiff = totdiff + diff\n",
    "\n",
    "    avediff = totdiff / len(prev_ranks)\n",
    "    print(i+1, avediff)\n",
    "\n",
    "    # rotate\n",
    "    prev_ranks = next_ranks\n",
    "\n",
    "# Put the final ranks back into the database\n",
    "print(list(next_ranks.items())[:5])\n",
    "cur.execute('''UPDATE Pages SET old_rank=new_rank''')\n",
    "for (id, new_rank) in list(next_ranks.items()) :\n",
    "    cur.execute('''UPDATE Pages SET new_rank=? WHERE id=?''', (new_rank, id))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 8.685419190855981, 8.685419190855981, 1, 'http://yellowpages.in')\n",
      "(163, 7.7203726140942015, 7.7203726140942015, 2, 'http://yellowpages.in/addbusiness')\n",
      "(132, 8.93357402488044, 8.93357402488044, 3, 'http://yellowpages.in/categories')\n",
      "(100, 7.720372614094201, 7.720372614094201, 45, 'http://yellowpages.in/contact-us')\n",
      "(67, 7.7203726140942015, 7.7203726140942015, 49, 'http://yellowpages.in/terms-conditions')\n",
      "(65, 7.7203726140942, 7.7203726140942, 43, 'http://yellowpages.in/advertise-with-us')\n",
      "(11, 0.197074171024989, 0.197074171024989, 93, 'http://yellowpages.in/b/van-heusen-clothing-himayat-nagar-hyderabad/381185220')\n",
      "(10, 0.68625534347504, 0.68625534347504, 35, 'http://yellowpages.in/b/navata-road-transport-mehdipatnam-hyderabad/860597192')\n",
      "(10, 0.38524193325862405, 0.38524193325862405, 560, 'http://yellowpages.in/hyderabad/cutlery/704794315')\n",
      "(9, 0.6137914007678065, 0.6137914007678065, 17, 'http://yellowpages.in/b/khadims-foot-wear-m-g-road-hyderabad/246975848')\n",
      "(9, 0.16870023844737508, 0.16870023844737508, 88, 'http://yellowpages.in/b/me-n-moms-baby-store-kukatpally-hyderabad/146211981')\n",
      "(9, 0.16870023844737514, 0.16870023844737514, 92, 'http://yellowpages.in/b/wills-life-style-showroom-himayat-nagar-hyderabad/382437791')\n",
      "(8, 0.17156383586875992, 0.17156383586875992, 102, 'http://yellowpages.in/b/fedex-couriers-mehdipatnam-hyderabad/523585508')\n",
      "(7, 0.05579921825161859, 0.05579921825161859, 78, 'http://yellowpages.in/offers/khadims-foot-wear-m-g-road-hyderabad/246975848')\n",
      "(7, 0.3845255506097498, 0.3845255506097498, 907, 'http://yellowpages.in/hyderabad/pizza-restaurants/681611649')\n",
      "(6, 0.579027946057065, 0.579027946057065, 39, 'http://yellowpages.in/b/it-mall-khairatabad-hyderabad/482883833')\n",
      "(6, 0.579027946057065, 0.579027946057065, 41, 'http://yellowpages.in/b/samsung-mobile-service-center-tarnaka-hyderabad/492237229')\n",
      "(6, 0.08290627409453415, 0.08290627409453415, 66, 'http://yellowpages.in/b/chitrakala-art-academy-manikonda-hyderabad/504801824')\n",
      "(6, 0.0842222466992093, 0.0842222466992093, 67, 'http://yellowpages.in/b/sri-tyagaraja-sangeetha-kalakshetram-kukatpally-hyderabad/941181735')\n",
      "(6, 0.3859197783024809, 0.3859197783024809, 681, 'http://yellowpages.in/hyderabad/glasswares/339277418')\n",
      "(6, 0.048252328838088575, 0.048252328838088575, 1212, 'http://yellowpages.in/b/karthikeya-infominds-kukatpally-hyderabad/230032215')\n",
      "(6, 0.048239972287809935, 0.048239972287809935, 1254, 'http://yellowpages.in/b/meeras-home-appliance-himayat-nagar-hyderabad/958563492')\n",
      "(6, 0.042111123349604554, 0.042111123349604554, 1736, 'http://yellowpages.in/b/v-green-media-advertising-and-branding--himayat-nagar-hyderabad/469689983')\n",
      "(6, 0.047548858408732325, 0.047548858408732325, 1832, 'http://yellowpages.in/b/new-akbar-restaurant-mehdipatnam-hyderabad/611695853')\n",
      "(6, 0.04272506117886091, 0.04272506117886091, 1901, 'http://yellowpages.in/b/burgers-and-subs-miyapur-hyderabad/696788060')\n",
      "(5, 0.579027946057065, 0.579027946057065, 15, 'http://yellowpages.in/b/vandana-silk-centre-chikkadpally-hyderabad/421201496')\n",
      "(5, 0.579027946057065, 0.579027946057065, 22, 'http://yellowpages.in/b/designers-institute-chembur-east-mumbai/549980075')\n",
      "(5, 0.579027946057065, 0.579027946057065, 24, 'http://yellowpages.in/b/jyothi-interiors-nagole-hyderabad/540672443')\n",
      "(5, 0.579027946057065, 0.579027946057065, 38, 'http://yellowpages.in/reviews/dtdc-service-chanda-nagar-hyderabad/775682829')\n",
      "(5, 0.08271827800815199, 0.08271827800815199, 283, 'http://yellowpages.in/b/sri-shyam-sanitary-hardware-and-paints-bandlaguda-jagir-hyderabad/224643309')\n",
      "(5, 0.08271827800815199, 0.08271827800815199, 291, 'http://yellowpages.in/b/metro-mobiles-himayat-nagar-hyderabad/746466256')\n",
      "(5, 0.3803908672698601, 0.3803908672698601, 653, 'http://yellowpages.in/hyderabad/food-and-beverage-outlets/950485848')\n",
      "(4, 0.579027946057065, 0.579027946057065, 42, 'http://yellowpages.in/b/sri-lakshmi-graphics-chikkadpally-hyderabad/651147200')\n",
      "(4, 0.40856946039811504, 0.40856946039811504, 83, 'http://yellowpages.in/hyderabad/sandals-and-floaters/423462435')\n",
      "(4, 0.38423974300561, 0.38423974300561, 344, 'http://yellowpages.in/categories?s=a')\n",
      "(4, 0.3735664168110097, 0.3735664168110097, 345, 'http://yellowpages.in/categories?s=b')\n",
      "(4, 0.33776630186662127, 0.33776630186662127, 1127, 'http://yellowpages.in/hyderabad/web-designing-companies/280749688')\n",
      "(3, 0.10722739741797485, 0.10722739741797485, 109, 'http://yellowpages.in/hyderabad/logistics-services-in-mehdipatnam/195433585')\n",
      "(3, 0.3735664168110097, 0.3735664168110097, 401, 'http://yellowpages.in/hyderabad/anglican-churches/306500983')\n",
      "(3, 0.3735664168110097, 0.3735664168110097, 409, 'http://yellowpages.in/hyderabad/arms-and-ammunition-dealer/496198965')\n",
      "(3, 0.3368889867968378, 0.3368889867968378, 800, 'http://yellowpages.in/hyderabad/media-advertising/531353738')\n",
      "(3, 0.34140089287000974, 0.34140089287000974, 834, 'http://yellowpages.in/hyderabad/music-academies/138201787')\n",
      "(3, 0.04903079150564289, 0.04903079150564289, 1426, 'http://yellowpages.in/hyderabad/cutlery-in-nagaram/704794315')\n",
      "(2, 0.579027946057065, 0.579027946057065, 34, 'http://yellowpages.in/locations/white-house-apparels-pvt-ltd/869060096')\n",
      "(2, 0.029183532885579448, 0.029183532885579448, 238, 'http://yellowpages.in/hyderabad/sandals-and-floaters/tel:9652020881')\n",
      "(2, 0.029183532885579448, 0.029183532885579448, 244, 'http://yellowpages.in/hyderabad/sandals-and-floaters/tel:9985949302')\n",
      "(2, 0.029183532885579448, 0.029183532885579448, 251, 'http://yellowpages.in/hyderabad/sandals-and-floaters/tel:8978167525')\n",
      "(2, 0.029183532885579448, 0.029183532885579448, 253, 'http://yellowpages.in/hyderabad/sandals-and-floaters/tel:9885086784')\n",
      "(2, 0.029183532885579448, 0.029183532885579448, 257, 'http://yellowpages.in/hyderabad/sandals-and-floaters/tel:9703501013')\n",
      "(2, 0.029183532885579448, 0.029183532885579448, 260, 'http://yellowpages.in/hyderabad/sandals-and-floaters/tel:9391013297')\n",
      "64 rows.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider2.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "     FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "     WHERE html IS NOT NULL\n",
    "     GROUP BY id ORDER BY inbound DESC''')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    if count < 50 : print(row)\n",
    "    count = count + 1\n",
    "print(count, 'rows.')\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating JSON output on spider.js...\n",
      "How many nodes? 25\n",
      "Open force.html in a browser to view the visualization\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider2.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Creating JSON output on spider.js...\")\n",
    "howmany = int(input(\"How many nodes? \"))\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "    FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "    WHERE html IS NOT NULL AND ERROR IS NULL\n",
    "    GROUP BY id ORDER BY id,inbound''')\n",
    "\n",
    "fhand = open('spider.js','w')\n",
    "nodes = list()\n",
    "maxrank = None\n",
    "minrank = None\n",
    "for row in cur :\n",
    "    nodes.append(row)\n",
    "    rank = row[2]\n",
    "    if maxrank is None or maxrank < rank: maxrank = rank\n",
    "    if minrank is None or minrank > rank : minrank = rank\n",
    "    if len(nodes) > howmany : break\n",
    "\n",
    "if maxrank == minrank or maxrank is None or minrank is None:\n",
    "    print(\"Error - please run sprank.py to compute page rank\")\n",
    "    quit()\n",
    "\n",
    "fhand.write('spiderJson = {\"nodes\":[\\n')\n",
    "count = 0\n",
    "map = dict()\n",
    "ranks = dict()\n",
    "for row in nodes :\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    # print row\n",
    "    rank = row[2]\n",
    "    rank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{'+'\"weight\":'+str(row[0])+',\"rank\":'+str(rank)+',')\n",
    "    fhand.write(' \"id\":'+str(row[3])+', \"url\":\"'+row[4]+'\"}')\n",
    "    map[row[3]] = count\n",
    "    ranks[row[3]] = rank\n",
    "    count = count + 1\n",
    "fhand.write('],\\n')\n",
    "\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "fhand.write('\"links\":[\\n')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    # print row\n",
    "    if row[0] not in map or row[1] not in map : continue\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    rank = ranks[row[0]]\n",
    "    srank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{\"source\":'+str(map[row[0]])+',\"target\":'+str(map[row[1]])+',\"value\":3}')\n",
    "    count = count + 1\n",
    "fhand.write(']};')\n",
    "fhand.close()\n",
    "cur.close()\n",
    "\n",
    "print(\"Open force.html in a browser to view the visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "[4, 8]\n",
      "[4, 8, 16]\n",
      "3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-da59dd10e991>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-da59dd10e991>\u001b[0m in \u001b[0;36mfact\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m*=\u001b[0m\u001b[0mfact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mn\u001b[0m\u001b[1;33m-=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[1;32m<ipython-input-17-da59dd10e991>\u001b[0m in \u001b[0;36mfact\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m*=\u001b[0m\u001b[0mfact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mn\u001b[0m\u001b[1;33m-=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "def fact(n):\n",
    "    x=1\n",
    "    if n>1:\n",
    "        x*=fact(n)\n",
    "        n-=1\n",
    "    return x\n",
    "\n",
    "n=int(input())\n",
    "\n",
    "y=fact(n)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The factorial of 25 is 15511210043330985984000000\n"
     ]
    }
   ],
   "source": [
    "# Python program to find the factorial of a number provided by the user.\n",
    "\n",
    "# change the value for a different result\n",
    "num = 25\n",
    "\n",
    "# To take input from the user\n",
    "#num = int(input(\"Enter a number: \"))\n",
    "\n",
    "factorial = 1\n",
    "\n",
    "# check if the number is negative, positive or zero\n",
    "if num < 0:\n",
    "   print(\"Sorry, factorial does not exist for negative numbers\")\n",
    "elif num == 0:\n",
    "   print(\"The factorial of 0 is 1\")\n",
    "else:\n",
    "   for i in range(1,num + 1):\n",
    "       factorial = factorial*i\n",
    "   print(\"The factorial of\",num,\"is\",factorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
